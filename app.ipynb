{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROHME 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 18:54:06.716155: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 18:54:06.726276: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734317646.738081   57338 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734317646.741298   57338 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 18:54:06.751836: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `crohme_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734317650.290385   57338 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4968 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import datasets.crohme_dataset  # Register `crohme_dataset`\n",
    "\n",
    "ds = tfds.load(\"crohme_dataset\")  # `crohme_dataset` registered\n",
    "test: tf.data.Dataset = ds[\"test\"]\n",
    "train: tf.data.Dataset = ds[\"train\"]\n",
    "validation: tf.data.Dataset = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Previewing InkML Files\n",
    "\n",
    "I also created a little utility in C++ and GTK to render out an inkml file from the dataset. It reads the InkML file, and renders out the strokes as well as the LaTeX of what it's supposed to be. It was a fun project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 18:54:12.506560: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeshwinprince/Programming/crohme/datasets/crohme_dataset/data/INKML/val/CROHME2016_test/UN_119_em_412.inkml\n",
      "Displaying app now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "random_data_point = next(iter(validation.shuffle(200_000).take(1)))\n",
    "filepath = random_data_point[\"filepath\"].numpy().decode(\"ascii\")\n",
    "os.system(f\"inkmlviewer {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Text Vectorization\n",
    "\n",
    "We will use `pylatexenc` to parse the LaTeX into nodes for custom splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatexenc.latexwalker import (\n",
    "    LatexWalker,\n",
    "    LatexMacroNode,\n",
    "    LatexEnvironmentNode,\n",
    "    LatexCharsNode,\n",
    "    LatexGroupNode,\n",
    ")\n",
    "\n",
    "START_TOKEN, END_TOKEN = \"<START>\", \"<END>\"\n",
    "\n",
    "\n",
    "# Define the tokenization function using pylatexenc\n",
    "def latex_tokenizer(latex_string):\n",
    "    \"\"\"\n",
    "    Tokenizes a LaTeX string into tokens using pylatexenc.\n",
    "    \"\"\"\n",
    "    if not latex_string:\n",
    "        return []\n",
    "    walker = LatexWalker(latex_string)\n",
    "\n",
    "    def parse_node(nodelist):\n",
    "        if len(nodelist) == 0:\n",
    "            return []\n",
    "        try:\n",
    "            tokens = []\n",
    "            for node in nodelist:\n",
    "                if not node:\n",
    "                    continue\n",
    "                elif node.isNodeType(LatexMacroNode):\n",
    "                    tokens.append(f\"\\\\{node.macroname}\")\n",
    "                    # Parse arguments if they exist\n",
    "                    tokens += parse_node(node.nodeargd.argnlist)\n",
    "                elif node.isNodeType(LatexEnvironmentNode):\n",
    "                    tokens.append(f\"\\\\begin{{{node.environmentname}}}\")\n",
    "                    tokens += parse_node(node.nodeargd.argnlist)\n",
    "                    tokens += parse_node(node.nodelist)\n",
    "                    tokens.append(f\"\\\\end{{{node.environmentname}}}\")\n",
    "                elif node.isNodeType(LatexCharsNode):\n",
    "                    tokens += list(node.chars)\n",
    "                elif node.isNodeType(LatexGroupNode):\n",
    "                    tokens.append(node.delimiters[0])\n",
    "                    tokens += parse_node(node.nodelist)\n",
    "                    tokens.append(node.delimiters[1])\n",
    "            return tokens\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "    nodelist, _, _ = walker.get_latex_nodes()\n",
    "    return parse_node(nodelist)\n",
    "\n",
    "\n",
    "# Wrap the tokenizer for use in TextVectorization\n",
    "def tokenize_fn(latex_tensor):\n",
    "    tokens = []\n",
    "    for latex_string in latex_tensor:\n",
    "        tokenized_string = latex_tokenizer(latex_string.numpy().decode(\"utf-8\"))\n",
    "        tokenized_string.insert(0, START_TOKEN)\n",
    "        tokenized_string.append(END_TOKEN)\n",
    "        tokens.append(tokenized_string)\n",
    "    return tf.ragged.constant(tokens, dtype=tf.string)\n",
    "\n",
    "\n",
    "# Create a TensorFlow-compatible wrapper\n",
    "@tf.function\n",
    "def tf_tokenizer(latex_string):\n",
    "    return tf.py_function(\n",
    "        func=tokenize_fn,\n",
    "        inp=[latex_string],\n",
    "        Tout=tf.RaggedTensorSpec([None, None], dtype=tf.string),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vectorizer and use a vocabulary file to adapt it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 18:38:48.691008: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Create the TextVectorization layer\n",
    "max_tokens = 10000  # Adjust depending on your vocabulary size\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    standardize=None,  # Custom tokenizer, so no built-in preprocessing\n",
    "    split=tf_tokenizer,\n",
    "    ragged=True,\n",
    ")\n",
    "dataset = tf.data.TextLineDataset(\"vocabulary.txt\")\n",
    "dataset = dataset.map(lambda line: [line])\n",
    "vectorizer.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out to make sure it works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[4, 60, 45, 11, 45, 30, 39, 9, 12, 5],\n",
      " [4, 20, 3, 21, 2, 3, 38, 2, 45, 18, 45, 79, 3, 39, 2, 5],\n",
      " [4, 73, 6, 3, 19, 11, 10, 2, 9, 16, 45, 19, 9, 12, 45, 11, 45, 20, 3, 16,\n",
      "  7, 16, 18, 10, 8, 7, 12, 16, 18, 10, 8, 2, 3, 92, 2, 5]                 ,\n",
      " [4, 34, 45, 11, 45, 78, 27, 9, 12, 5],\n",
      " [4, 76, 11, 165, 10, 154, 10, 17, 154, 17, 75, 45, 245, 245, 245, 76, 9, 3,\n",
      "  69, 2, 75, 45, 164, 5]                                                    ]>\n",
      "<START>E = mc^2<END>\n"
     ]
    }
   ],
   "source": [
    "def latex_to_token(string):\n",
    "    return vectorizer(string)\n",
    "\n",
    "\n",
    "id_to_token = {i: token for i, token in enumerate(vectorizer.get_vocabulary())}\n",
    "\n",
    "\n",
    "def token_to_latex(tokens):\n",
    "    return \"\".join([id_to_token[id] for id in tokens.numpy()])\n",
    "\n",
    "\n",
    "latex_array = [\n",
    "    r\"E = mc^2\",\n",
    "    r\"\\frac{a}{b} + \\sqrt{c}\",\n",
    "    r\"\\sum_{i=1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}\",\n",
    "    r\"A = \\pi r^2\",\n",
    "    r\"G=\\begin{bmatrix}1&\\dots&1&0&\\dots&0\\\\ \\ast&\\ast&\\ast&&G^{\\prime}&\\\\ \\end{bmatrix}\",\n",
    "]\n",
    "latex_data = tf.constant(latex_array)\n",
    "\n",
    "# Tokenize and vectorize\n",
    "tokenized_output = latex_to_token(latex_data)\n",
    "print(tokenized_output)\n",
    "\n",
    "E_mc2 = token_to_latex(tokenized_output[0])\n",
    "print(E_mc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the config as a keras model just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Input(shape=(1,), dtype=tf.string))\n",
    "model.add(vectorizer)\n",
    "\n",
    "# Save\n",
    "filepath = \"notebook_vectorizer.keras\"\n",
    "model.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the vectorizer back in when we need it while running this notebook (NOT WORKING!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<class 'keras.src.models.sequential.Sequential'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras', 'class_name': 'Sequential', 'config': {'name': 'sequential_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 129980972466928}, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': [None, 1], 'dtype': 'string', 'sparse': False, 'name': 'input_layer_1'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TextVectorization', 'config': {'name': 'text_vectorization_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'max_tokens': 10000, 'standardize': None, 'split': {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}, 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': None, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': True, 'vocabulary': None, 'idf_weights': None, 'encoding': 'utf-8', 'vocabulary_size': 966}, 'registered_name': None, 'build_config': {'input_shape': [5]}}], 'build_input_shape': [None, 1]}, 'registered_name': None, 'build_config': {'input_shape': [None, 1]}}.\n\nException encountered: <class 'keras.src.layers.preprocessing.text_vectorization.TextVectorization'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.layers', 'class_name': 'TextVectorization', 'config': {'name': 'text_vectorization_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'max_tokens': 10000, 'standardize': None, 'split': {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}, 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': None, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': True, 'vocabulary': None, 'idf_weights': None, 'encoding': 'utf-8', 'vocabulary_size': 966}, 'registered_name': None, 'build_config': {'input_shape': [5]}}.\n\nException encountered: Unable to reconstruct an instance of 'Function' because the class is missing a `from_config()` method. Full object config: {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/layers/preprocessing/text_vectorization.py:490\u001b[0m, in \u001b[0;36mTextVectorization.from_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 490\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mngrams\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:706\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_config\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 706\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to reconstruct an instance of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe class is missing a `from_config()` method. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    710\u001b[0m     )\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Instantiate the class from its config inside a custom object scope\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# so that we can catch any custom objects that the config refers to.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unable to reconstruct an instance of 'Function' because the class is missing a `from_config()` method. Full object config: {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:718\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/models/sequential.py:355\u001b[0m, in \u001b[0;36mSequential.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[43mserialization_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    359\u001b[0m model\u001b[38;5;241m.\u001b[39madd(layer)\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:720\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instances (layers, models, etc.) returned by\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `get_config()` are explicitly deserialized in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms `from_config()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m     )\n\u001b[1;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.layers.preprocessing.text_vectorization.TextVectorization'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.layers', 'class_name': 'TextVectorization', 'config': {'name': 'text_vectorization_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'max_tokens': 10000, 'standardize': None, 'split': {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}, 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': None, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': True, 'vocabulary': None, 'idf_weights': None, 'encoding': 'utf-8', 'vocabulary_size': 966}, 'registered_name': None, 'build_config': {'input_shape': [5]}}.\n\nException encountered: Unable to reconstruct an instance of 'Function' because the class is missing a `from_config()` method. Full object config: {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m loaded_vectorizer \u001b[38;5;241m=\u001b[39m loaded_model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/saving_api.py:189\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    186\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[1;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[1;32m    198\u001b[0m     )\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:367\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m     )\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:444\u001b[0m, in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zf\u001b[38;5;241m.\u001b[39mopen(_CONFIG_FILENAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    442\u001b[0m     config_json \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 444\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    449\u001b[0m extract_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/saving_lib.py:433\u001b[0m, in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 433\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/keras/src/saving/serialization_lib.py:720\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m     instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_config(inner_config)\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m instances (layers, models, etc.) returned by\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    724\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `get_config()` are explicitly deserialized in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms `from_config()` method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m     )\n\u001b[1;32m    728\u001b[0m build_config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuild_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m build_config \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mbuilt:\n",
      "\u001b[0;31mTypeError\u001b[0m: <class 'keras.src.models.sequential.Sequential'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras', 'class_name': 'Sequential', 'config': {'name': 'sequential_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None, 'shared_object_id': 129980972466928}, 'layers': [{'module': 'keras.layers', 'class_name': 'InputLayer', 'config': {'batch_shape': [None, 1], 'dtype': 'string', 'sparse': False, 'name': 'input_layer_1'}, 'registered_name': None}, {'module': 'keras.layers', 'class_name': 'TextVectorization', 'config': {'name': 'text_vectorization_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'max_tokens': 10000, 'standardize': None, 'split': {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}, 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': None, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': True, 'vocabulary': None, 'idf_weights': None, 'encoding': 'utf-8', 'vocabulary_size': 966}, 'registered_name': None, 'build_config': {'input_shape': [5]}}], 'build_input_shape': [None, 1]}, 'registered_name': None, 'build_config': {'input_shape': [None, 1]}}.\n\nException encountered: <class 'keras.src.layers.preprocessing.text_vectorization.TextVectorization'> could not be deserialized properly. Please ensure that components that are Python object instances (layers, models, etc.) returned by `get_config()` are explicitly deserialized in the model's `from_config()` method.\n\nconfig={'module': 'keras.layers', 'class_name': 'TextVectorization', 'config': {'name': 'text_vectorization_1', 'trainable': True, 'dtype': {'module': 'keras', 'class_name': 'DTypePolicy', 'config': {'name': 'float32'}, 'registered_name': None}, 'max_tokens': 10000, 'standardize': None, 'split': {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}, 'ngrams': None, 'output_mode': 'int', 'output_sequence_length': None, 'pad_to_max_tokens': False, 'sparse': False, 'ragged': True, 'vocabulary': None, 'idf_weights': None, 'encoding': 'utf-8', 'vocabulary_size': 966}, 'registered_name': None, 'build_config': {'input_shape': [5]}}.\n\nException encountered: Unable to reconstruct an instance of 'Function' because the class is missing a `from_config()` method. Full object config: {'module': 'tensorflow.python.eager.polymorphic_function.polymorphic_function', 'class_name': 'Function', 'config': 'tf_tokenizer', 'registered_name': 'Function'}"
     ]
    }
   ],
   "source": [
    "# loaded_model = tf.keras.models.load_model(filepath)\n",
    "# loaded_vectorizer = loaded_model.layers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Strokes\n",
    "\n",
    "Instead of images, this model takes in a stream of strokes, such as writing with a stylus on a tablet. Our dataset gives us a list of strokes, and each stroke is itself a list of coordinates [x, y] of the position of the stylus. Both the number of strokes and the length of each strokes changes for every value in our dataset, so we are going to pre-process the stroke data so it will be normalized (scaled to be between 0 and 1), and always fit in a tensor with shape `(64, 64, 2,)`. FOr this, I am using the [Ramer-Douglas-Peucker Algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm) for polyline decimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_strokes(strokes: tf.RaggedTensor):\n",
    "    # First, scale values to between 0.0 and 1.0\n",
    "    min_vals = tf.reduce_min(strokes, axis=(0, 1))\n",
    "    max_vals = tf.reduce_max(strokes, axis=(0, 1))\n",
    "    normalized_strokes = tf.map_fn(\n",
    "        elems=strokes,\n",
    "        fn=lambda stroke: (stroke - min_vals) / (max_vals - min_vals + 1e-6),\n",
    "    )\n",
    "\n",
    "    def point_line_distance(point, start, end):\n",
    "        \"\"\"\n",
    "        Calculate the perpendicular distance from `point` to the line segment\n",
    "        defined by `start` and `end`.\n",
    "        \"\"\"\n",
    "         # Convert to 3D by adding a zero z-component\n",
    "        point_3d = tf.concat([point, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
    "        start_3d = tf.concat([start, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
    "        end_3d = tf.concat([end, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
    "        \n",
    "        # Compute the cross product between the vectors\n",
    "        cross_prod = tf.linalg.cross(end_3d - start_3d, point_3d - start_3d)\n",
    "        \n",
    "        # Return the perpendicular distance (norm of the cross product / norm of the line segment)\n",
    "        return tf.norm(cross_prod) / tf.norm(end_3d - start_3d)\n",
    "\n",
    "    def douglas_peucker(stroke, epsilon=0.01):\n",
    "        \"\"\"\n",
    "        Non-recursive Douglas-Peucker algorithm implementation.\n",
    "        \"\"\"\n",
    "        stroke_len = tf.shape(stroke)[0]\n",
    "        if stroke_len < 3:\n",
    "            return stroke\n",
    "\n",
    "        # Initialize the list of points to keep\n",
    "        simplified_stroke = [stroke[0]]\n",
    "\n",
    "        # Stack for processing: Each entry contains a tuple (start_index, end_index)\n",
    "        stack = [(0, stroke_len - 1)]\n",
    "\n",
    "        while stack:\n",
    "            start_idx, end_idx = stack.pop()\n",
    "\n",
    "            # Get the relevant slice of the stroke\n",
    "            sub_stroke = stroke[start_idx : end_idx + 1]\n",
    "\n",
    "            # Calculate the perpendicular distances of all intermediate points\n",
    "            start, end = sub_stroke[0], sub_stroke[-1]\n",
    "            distances = tf.vectorized_map(\n",
    "                lambda p: point_line_distance(p, start, end), sub_stroke[1:-1]\n",
    "            )\n",
    "\n",
    "            # Find the point with the maximum distance\n",
    "            if tf.size(distances) > 0:\n",
    "                max_distance = tf.reduce_max(distances)\n",
    "                max_idx = tf.argmax(distances) + 1  # +1 because we skip the start point\n",
    "\n",
    "                # If the max distance is greater than epsilon, continue splitting\n",
    "                if max_distance > epsilon:\n",
    "                    stack.append((start_idx, start_idx + max_idx))\n",
    "                    stack.append((start_idx + max_idx, end_idx))\n",
    "                else:\n",
    "                    # Otherwise, keep the start and end points\n",
    "                    simplified_stroke.append(end)\n",
    "            else:\n",
    "                # If no intermediate points exist, just keep the start and end points\n",
    "                simplified_stroke.append(end)\n",
    "\n",
    "        # Return the simplified stroke\n",
    "        return tf.stack(simplified_stroke)\n",
    "\n",
    "    downsampled_strokes = tf.map_fn(elems=normalized_strokes, fn=douglas_peucker)\n",
    "\n",
    "    # Pad to fixed number of strokes and points per stroke\n",
    "    return downsampled_strokes.to_tensor(shape=(64, 64, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7a5914bf9800> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7a5914bf9800> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7a5909fbcc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function pfor.<locals>.f at 0x7a5909fbcc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.         0.42909095]\n",
      "  [0.09121891 0.46363685]\n",
      "  [0.06434508 0.4181823 ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.21158199 0.80727303]\n",
      "  [0.29598787 0.9999998 ]\n",
      "  [0.2853899  0.03090875]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.36866003 0.8636362 ]\n",
      "  [0.36222556 0.92363656]\n",
      "  [0.35011354 0.94363666]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]]\n",
      "\n",
      " [[0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  ...\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.         0.        ]]], shape=(64, 64, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "t = next(iter(validation.take(1)))\n",
    "print(preprocess_strokes(t[\"strokes\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing datasets\n",
    "\n",
    "Now, we can go through our datasets a preprocess all the data. We will need both our vectorizer and our stroke preprocessor together. Since we are going with an encoder-decoder model, we need input data for the decoder as well, which should be the desired output, but just missing the last token, and with the start token added in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    input_strokes = preprocess_strokes(data[\"strokes\"])\n",
    "    ground_truth = vectorizer(data[\"ground_truth\"])\n",
    "    decoder_input = ground_truth[:-1]\n",
    "    decoder_output = ground_truth[1:]\n",
    "    return (input_strokes, decoder_input), decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m200_000\u001b[39m)\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m5\u001b[39m)):\n\u001b[0;32m----> 2\u001b[0m     pp_data \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# print(pp_data)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder input:\u001b[39m\u001b[38;5;124m\"\u001b[39m, token_to_latex(pp_data[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m, in \u001b[0;36mpreprocess_data\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_data\u001b[39m(data):\n\u001b[1;32m      2\u001b[0m     input_strokes \u001b[38;5;241m=\u001b[39m preprocess_strokes(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrokes\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m(data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mground_truth\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      4\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m ground_truth[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m     decoder_output \u001b[38;5;241m=\u001b[39m ground_truth[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test.shuffle(200_000).take(5)):\n",
    "    pp_data = preprocess_data(data)\n",
    "    # print(pp_data)\n",
    "    print(\"Decoder input:\", token_to_latex(pp_data[0][1]))\n",
    "    print(\"True value:\", token_to_latex(pp_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.map(preprocess_data).shuffle(200_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train = train.map(preprocess_data).shuffle(200_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "validation = (\n",
    "    validation.map(preprocess_data)\n",
    "    .shuffle(200_000)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out to make sure it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 1. First element had shape [35] and element 1 had shape [11]. [Op:IteratorGetNext] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[175], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:826\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    825\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    827\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/iterator_ops.py:776\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 776\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3086\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3086\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3087\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   3088\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/Programming/crohme/.venv/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:6002\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6001\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6002\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} Cannot batch tensors with different shapes in component 1. First element had shape [35] and element 1 had shape [11]. [Op:IteratorGetNext] name: "
     ]
    }
   ],
   "source": [
    "print(next(iter(validation.take(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "My model is an encoder-decoder architecture, with a CNN for the encoder, a feedforward network to get to the latent space, and a LSTM RNN for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strokes = layers.Input(shape=(64, 64, 2))\n",
    "x = layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\")(input_strokes)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "latent_space = layers.Dense(1024, activation=\"relu\")(x)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_h = layers.Dense(256, activation=\"relu\")(latent_space)\n",
    "latent_space_c = layers.Dense(256, activation=\"relu\")(latent_space)\n",
    "\n",
    "decoder_input = layers.Input(shape=(None,))\n",
    "decoder_embedding = layers.Embedding(input_dim=vectorizer.vocabulary_size(), output_dim=128)(\n",
    "    decoder_input\n",
    ")\n",
    "decoder_lstm = layers.LSTM(128, return_sequences=True)\n",
    "decoder_output = decoder_lstm(\n",
    "    decoder_embedding, initial_state=[latent_space_h, latent_space_c]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = layers.Dense(vectorizer.vocabulary_size(), activation=\"softmax\")(decoder_output)\n",
    "model = keras.Model([input_strokes, decoder_input], output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we can finally train our model! We have a ton of training and validation data to use. We'll also save the history so we can get a graph of the change in loss over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=validation,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\"model_checkpoint.h5\", save_best_only=True),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
