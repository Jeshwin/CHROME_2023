{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CROHME 2023"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import tensorflow_datasets as tfds\n",
                "import keras\n",
                "from keras import layers\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Import `crohme_dataset`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import datasets.crohme_dataset  # Register `crohme_dataset`\n",
                "\n",
                "ds = tfds.load(\"crohme_dataset\")  # `crohme_dataset` registered\n",
                "test: tf.data.Dataset = ds[\"test\"]\n",
                "train: tf.data.Dataset = ds[\"train\"]\n",
                "validation: tf.data.Dataset = ds[\"validation\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Extra: Previewing InkML Files\n",
                "\n",
                "I also created a little utility in C++ and GTK to render out an inkml file from the dataset. It reads the InkML file, and renders out the strokes as well as the LaTeX of what it's supposed to be. It was a fun project!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "random_data_point = next(iter(validation.shuffle(200_000).take(1)))\n",
                "filepath = random_data_point[\"filepath\"].numpy().decode(\"ascii\")\n",
                "os.system(f\"inkmlviewer {filepath}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocessing\n",
                "\n",
                "### Text Vectorization\n",
                "\n",
                "We will use `pylatexenc` to parse the LaTeX into nodes for custom splitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pylatexenc.latexwalker import (\n",
                "    LatexWalker,\n",
                "    LatexMacroNode,\n",
                "    LatexEnvironmentNode,\n",
                "    LatexCharsNode,\n",
                "    LatexGroupNode,\n",
                ")\n",
                "\n",
                "START_TOKEN, END_TOKEN = \"<START>\", \"<END>\"\n",
                "\n",
                "\n",
                "# Define the tokenization function using pylatexenc\n",
                "def latex_tokenizer(latex_string):\n",
                "    \"\"\"\n",
                "    Tokenizes a LaTeX string into tokens using pylatexenc.\n",
                "    \"\"\"\n",
                "    if not latex_string:\n",
                "        return []\n",
                "    walker = LatexWalker(latex_string)\n",
                "\n",
                "    def parse_node(nodelist):\n",
                "        if len(nodelist) == 0:\n",
                "            return []\n",
                "        try:\n",
                "            tokens = []\n",
                "            for node in nodelist:\n",
                "                if not node:\n",
                "                    continue\n",
                "                elif node.isNodeType(LatexMacroNode):\n",
                "                    tokens.append(f\"\\\\{node.macroname}\")\n",
                "                    # Parse arguments if they exist\n",
                "                    tokens += parse_node(node.nodeargd.argnlist)\n",
                "                elif node.isNodeType(LatexEnvironmentNode):\n",
                "                    tokens.append(f\"\\\\begin{{{node.environmentname}}}\")\n",
                "                    tokens += parse_node(node.nodeargd.argnlist)\n",
                "                    tokens += parse_node(node.nodelist)\n",
                "                    tokens.append(f\"\\\\end{{{node.environmentname}}}\")\n",
                "                elif node.isNodeType(LatexCharsNode):\n",
                "                    tokens += list(node.chars)\n",
                "                elif node.isNodeType(LatexGroupNode):\n",
                "                    tokens.append(node.delimiters[0])\n",
                "                    tokens += parse_node(node.nodelist)\n",
                "                    tokens.append(node.delimiters[1])\n",
                "            return tokens\n",
                "        except Exception as e:\n",
                "            return []\n",
                "\n",
                "    nodelist, _, _ = walker.get_latex_nodes()\n",
                "    return parse_node(nodelist)\n",
                "\n",
                "\n",
                "# Wrap the tokenizer for use in TextVectorization\n",
                "def tokenize_fn(latex_tensor):\n",
                "    tokens = []\n",
                "    for latex_string in latex_tensor:\n",
                "        tokenized_string = latex_tokenizer(latex_string.numpy().decode(\"utf-8\"))\n",
                "        tokenized_string.insert(0, START_TOKEN)\n",
                "        tokenized_string.append(END_TOKEN)\n",
                "        tokens.append(tokenized_string)\n",
                "    return tf.ragged.constant(tokens, dtype=tf.string)\n",
                "\n",
                "\n",
                "# Create a TensorFlow-compatible wrapper\n",
                "def tf_tokenizer(latex_string):\n",
                "    return tf.py_function(\n",
                "        func=tokenize_fn,\n",
                "        inp=[latex_string],\n",
                "        Tout=tf.RaggedTensorSpec([None, None], dtype=tf.string),\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Create the vectorizer and use a vocabulary file to adapt it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create the TextVectorization layer\n",
                "max_tokens = 10_000  # Adjust depending on your vocabulary size\n",
                "\n",
                "vectorizer = layers.TextVectorization(\n",
                "    max_tokens=max_tokens,\n",
                "    standardize=None,  # Custom tokenizer, so no built-in preprocessing\n",
                "    split=tf_tokenizer,\n",
                "    ragged=True,\n",
                ")\n",
                "dataset = tf.data.TextLineDataset(\"vocabulary.txt\")\n",
                "dataset = dataset.map(lambda line: [line])\n",
                "vectorizer.adapt(dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Test it out to make sure it works properly"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def latex_to_token(string):\n",
                "    return vectorizer(string)\n",
                "\n",
                "\n",
                "id_to_token = {i: token for i, token in enumerate(vectorizer.get_vocabulary())}\n",
                "\n",
                "\n",
                "def token_to_latex(tokens):\n",
                "    return \"\".join([id_to_token[id] for id in tokens.numpy()])\n",
                "\n",
                "\n",
                "latex_array = [\n",
                "    r\"E = mc^2\",\n",
                "    r\"\\frac{a}{b} + \\sqrt{c}\",\n",
                "    r\"\\sum_{i=1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}\",\n",
                "    r\"A = \\pi r^2\",\n",
                "    r\"G=\\begin{bmatrix}1&\\dots&1&0&\\dots&0\\\\ \\ast&\\ast&\\ast&&G^{\\prime}&\\\\ \\end{bmatrix}\",\n",
                "]\n",
                "latex_data = tf.constant(latex_array)\n",
                "\n",
                "# Tokenize and vectorize\n",
                "tokenized_output = latex_to_token(latex_data)\n",
                "print(tokenized_output)\n",
                "\n",
                "E_mc2 = token_to_latex(tokenized_output[0])\n",
                "print(E_mc2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Save the vocabulary just in case. You would still need the custom latex parser function imported to use it in another project"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open(\"vectorizer_vocabulary.txt\", \"w\") as f:\n",
                "    for word in vectorizer.get_vocabulary():\n",
                "        f.write(word)\n",
                "        f.write(\"\\n\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "To re-create the vectorizer using the vocabulary, run the following code"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [],
            "source": [
                "max_tokens = 10_000\n",
                "vectorizer = layers.TextVectorization(\n",
                "    max_tokens=max_tokens,\n",
                "    standardize=None,  # Custom tokenizer, so no built-in preprocessing\n",
                "    split=tf_tokenizer,\n",
                "    ragged=True,\n",
                ")\n",
                "with open(\"vectorizer_vocabulary.txt\", \"r\") as f:\n",
                "    lines = [line[:-1] for line in f]\n",
                "    vectorizer.set_vocabulary(lines)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Test it out to make sure it works properly"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def latex_to_token(string):\n",
                "    return vectorizer(string)\n",
                "\n",
                "\n",
                "id_to_token = {i: token for i, token in enumerate(vectorizer.get_vocabulary())}\n",
                "\n",
                "\n",
                "def token_to_latex(tokens):\n",
                "    return \"\".join([id_to_token[id] for id in tokens.numpy()])\n",
                "\n",
                "\n",
                "latex_array = [\n",
                "    r\"E = mc^2\",\n",
                "    r\"\\frac{a}{b} + \\sqrt{c}\",\n",
                "    r\"\\sum_{i=1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}\",\n",
                "    r\"A = \\pi r^2\",\n",
                "    r\"G=\\begin{bmatrix}1&\\dots&1&0&\\dots&0\\\\ \\ast&\\ast&\\ast&&G^{\\prime}&\\\\ \\end{bmatrix}\",\n",
                "]\n",
                "latex_data = tf.constant(latex_array)\n",
                "\n",
                "# Tokenize and vectorize\n",
                "tokenized_output = latex_to_token(latex_data)\n",
                "print(tokenized_output)\n",
                "\n",
                "E_mc2 = token_to_latex(tokenized_output[0])\n",
                "print(E_mc2)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Try an actual data point"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_data_point = next(iter(validation.take(1)))\n",
                "tokenized_data_point = latex_to_token([random_data_point['ground_truth'].numpy()])\n",
                "print(tokenized_data_point)\n",
                "detokenized_data_point = token_to_latex(tokenized_data_point[0])\n",
                "print(detokenized_data_point)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preprocessing Strokes\n",
                "\n",
                "Instead of images, this model takes in a stream of strokes, such as writing with a stylus on a tablet. Our dataset gives us a list of strokes, and each stroke is itself a list of coordinates [x, y] of the position of the stylus. Both the number of strokes and the length of each strokes changes for every value in our dataset, so we are going to pre-process the stroke data so it will be normalized (scaled to be between 0 and 1), and always fit in a tensor with shape `(64, 64, 2,)`. FOr this, I am using the [Ramer-Douglas-Peucker Algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm) for polyline decimation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 123,
            "metadata": {},
            "outputs": [],
            "source": [
                "@tf.function\n",
                "def preprocess_strokes(strokes: tf.RaggedTensor):\n",
                "    # First, scale values to between 0.0 and 1.0\n",
                "    min_vals = tf.reduce_min(strokes, axis=(0, 1))\n",
                "    max_vals = tf.reduce_max(strokes, axis=(0, 1))\n",
                "    normalized_strokes = tf.map_fn(\n",
                "        elems=strokes,\n",
                "        fn=lambda stroke: (stroke - min_vals) / (max_vals - min_vals + 1e-6),\n",
                "    )\n",
                "\n",
                "    def point_line_distance(point, start, end):\n",
                "        \"\"\"\n",
                "        Calculate the perpendicular distance from `point` to the line segment\n",
                "        defined by `start` and `end`.\n",
                "        \"\"\"\n",
                "        # Convert to 3D by adding a zero z-component\n",
                "        point_3d = tf.concat([point, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
                "        start_3d = tf.concat([start, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
                "        end_3d = tf.concat([end, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
                "\n",
                "        # Return the perpendicular distance (norm of the cross product / norm of the line segment)\n",
                "        return tf.norm(\n",
                "            tf.linalg.cross(end_3d - start_3d, point_3d - start_3d)\n",
                "        ) / tf.norm(end_3d - start_3d)\n",
                "\n",
                "    def douglas_peucker(stroke, epsilon=0.02):\n",
                "        \"\"\"\n",
                "        Non-recursive Douglas-Peucker algorithm implementation.\n",
                "        \"\"\"\n",
                "\n",
                "        # Initialize the list of points to keep\n",
                "        simplified_stroke: tf.TensorArray = tf.TensorArray(\n",
                "            dtype=stroke.dtype, size=0, dynamic_size=True\n",
                "        ).write(0, stroke[0])\n",
                "\n",
                "        # Stack for processing: Each entry contains a tuple (start_index, end_index)\n",
                "        stack: tf.TensorArray = tf.TensorArray(\n",
                "            dtype=tf.int32, size=0, dynamic_size=True\n",
                "        ).write(0, [0, tf.shape(stroke)[0] - 1])\n",
                "\n",
                "        def cond(stack: tf.TensorArray, _simplified_stroke: tf.TensorArray):\n",
                "            return tf.not_equal(stack.size(), 0)\n",
                "\n",
                "        def body(stack: tf.TensorArray, simplified_stroke: tf.TensorArray):\n",
                "            # Pop from the stack\n",
                "            border_idxs = stack.read(stack.size() - 1)\n",
                "            start_idx, end_idx = border_idxs[0], border_idxs[1]\n",
                "\n",
                "            if stack.size() > 1:\n",
                "                new_stack_tensor = stack.gather(tf.range(stack.size() - 1))\n",
                "                # Create a new stack with the remaining elements\n",
                "                new_stack = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
                "                for i in tf.range(stack.size() - 1):\n",
                "                    new_stack = new_stack.write(i, new_stack_tensor[i])\n",
                "                stack = new_stack\n",
                "            else:\n",
                "                stack = tf.TensorArray(dtype=tf.int32, size=0, dynamic_size=True)\n",
                "\n",
                "            # Get the relevant slice of the stroke\n",
                "            sub_stroke = stroke[start_idx : end_idx + 1]\n",
                "\n",
                "            # Calculate the perpendicular distances of all intermediate points\n",
                "            start, end = sub_stroke[0], sub_stroke[-1]\n",
                "            distances = tf.vectorized_map(\n",
                "                elems=sub_stroke[1:-1], fn=lambda p: point_line_distance(p, start, end)\n",
                "            )\n",
                "\n",
                "            if tf.size(distances) > 0:\n",
                "                max_distance = tf.reduce_max(distances)\n",
                "                max_idx = (\n",
                "                    tf.argmax(distances, output_type=tf.int32) + 1\n",
                "                )  # +1 because we skip the start point\n",
                "\n",
                "                # If the max distance is greater than epsilon, continue splitting\n",
                "                if max_distance > epsilon:\n",
                "                    stack = stack.write(stack.size(), [start_idx, start_idx + max_idx])\n",
                "                    stack = stack.write(stack.size(), [start_idx + max_idx, end_idx])\n",
                "                else:\n",
                "                    # Otherwise, keep the start and end points\n",
                "                    simplified_stroke = simplified_stroke.write(\n",
                "                        simplified_stroke.size(), end\n",
                "                    )\n",
                "            else:\n",
                "                # If no intermediate points exist, just keep the start and end points\n",
                "                simplified_stroke = simplified_stroke.write(\n",
                "                    simplified_stroke.size(), end\n",
                "                )\n",
                "\n",
                "            return stack, simplified_stroke\n",
                "\n",
                "        # Return the simplified stroke\n",
                "        return tf.while_loop(cond, body, [stack, simplified_stroke])[1].stack()\n",
                "\n",
                "    downsampled_strokes = tf.map_fn(elems=normalized_strokes, fn=douglas_peucker)\n",
                "    return downsampled_strokes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's try it out first!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_data_point = next(iter(validation.shuffle(100_000).take(1)))\n",
                "# print(random_data_point['strokes'])\n",
                "print(preprocess_strokes(random_data_point[\"strokes\"]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "random_data_point = next(iter(validation.shuffle(200_000).take(1)))\n",
                "print(random_data_point[\"strokes\"])\n",
                "print(preprocess_strokes(random_data_point[\"strokes\"]))\n",
                "# filepath = random_data_point[\"filepath\"].numpy().decode(\"ascii\")\n",
                "# os.system(f\"inkmlviewer {filepath}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Preprocessing datasets\n",
                "\n",
                "Now, we can go through our datasets a preprocess all the data. We will need both our vectorizer and our stroke preprocessor together. Since we are going with an encoder-decoder model, we need input data for the decoder as well, which should be the desired output, but just missing the last token, and with the start token added in front."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 103,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_data(data):\n",
                "    input_strokes = preprocess_strokes(data[\"strokes\"])\n",
                "    ground_truth = vectorizer([data[\"ground_truth\"]])\n",
                "    decoder_input = ground_truth[0][:-1]\n",
                "    decoder_output = ground_truth[0][1:]\n",
                "    return (input_strokes, decoder_input), decoder_output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for data in test.shuffle(200_000).take(5).map(preprocess_data):\n",
                "    # pp_data = preprocess_data(data)\n",
                "    # pp_data = data\n",
                "    print(\"Decoder input:\", token_to_latex(data[0][1]))\n",
                "    print(\"True value:\", token_to_latex(data[1]))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test = test.map(preprocess_data).shuffle(200_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
                "train = train.map(preprocess_data).shuffle(200_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
                "validation = (\n",
                "    validation.map(preprocess_data)\n",
                "    .shuffle(200_000)\n",
                "    .batch(32)\n",
                "    .prefetch(tf.data.AUTOTUNE)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's test this out to make sure it worked!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(next(iter(validation.take(1))))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Architecture\n",
                "\n",
                "My model is an encoder-decoder architecture, with a CNN for the encoder, a feedforward network to get to the latent space, and a LSTM RNN for the decoder."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Encoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_strokes = layers.Input(shape=(64, 64, 2))\n",
                "x = layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\")(input_strokes)\n",
                "x = layers.MaxPooling2D((2, 2))(x)\n",
                "x = layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\")(x)\n",
                "x = layers.MaxPooling2D((2, 2))(x)\n",
                "x = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\")(x)\n",
                "x = layers.MaxPooling2D((2, 2))(x)\n",
                "x = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\")(x)\n",
                "x = layers.MaxPooling2D((2, 2))(x)\n",
                "\n",
                "latent_space = layers.Dense(1024, activation=\"relu\")(x)\n",
                "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
                "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
                "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
                "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Decoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "latent_space_h = layers.Dense(256, activation=\"relu\")(latent_space)\n",
                "latent_space_c = layers.Dense(256, activation=\"relu\")(latent_space)\n",
                "\n",
                "decoder_input = layers.Input(shape=(None,))\n",
                "decoder_embedding = layers.Embedding(input_dim=vectorizer.vocabulary_size(), output_dim=128)(\n",
                "    decoder_input\n",
                ")\n",
                "decoder_lstm = layers.LSTM(128, return_sequences=True)\n",
                "decoder_output = decoder_lstm(\n",
                "    decoder_embedding, initial_state=[latent_space_h, latent_space_c]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Output"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "output = layers.Dense(vectorizer.vocabulary_size(), activation=\"softmax\")(decoder_output)\n",
                "model = keras.Model([input_strokes, decoder_input], output)\n",
                "model.compile(\n",
                "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training the Model\n",
                "\n",
                "Now we can finally train our model! We have a ton of training and validation data to use. We'll also save the history so we can get a graph of the change in loss over each epoch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = model.fit(\n",
                "    train,\n",
                "    validation_data=validation,\n",
                "    epochs=20,\n",
                "    verbose=1,\n",
                "    callbacks=[\n",
                "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
                "        tf.keras.callbacks.ModelCheckpoint(\"model_checkpoint.h5\", save_best_only=True),\n",
                "    ],\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
