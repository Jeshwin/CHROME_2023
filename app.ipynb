{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROHME 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 21:20:51.074829: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-15 21:20:51.183979: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734326451.231924    5360 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734326451.245444    5360 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-15 21:20:51.352751: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `crohme_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734326455.184715    5360 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5340 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import datasets.crohme_dataset  # Register `crohme_dataset`\n",
    "\n",
    "ds = tfds.load(\"crohme_dataset\")  # `crohme_dataset` registered\n",
    "test: tf.data.Dataset = ds[\"test\"]\n",
    "train: tf.data.Dataset = ds[\"train\"]\n",
    "validation: tf.data.Dataset = ds[\"validation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra: Previewing InkML Files\n",
    "\n",
    "I also created a little utility in C++ and GTK to render out an inkml file from the dataset. It reads the InkML file, and renders out the strokes as well as the LaTeX of what it's supposed to be. It was a fun project!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 21:20:57.502184: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:376] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jeshwinprince/Programming/crohme/datasets/crohme_dataset/data/INKML/val/CROHME2016_test/UN_451_em_619.inkml\n",
      "Displaying app now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "random_data_point = next(iter(validation.shuffle(200_000).take(1)))\n",
    "filepath = random_data_point[\"filepath\"].numpy().decode(\"ascii\")\n",
    "os.system(f\"inkmlviewer {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Text Vectorization\n",
    "\n",
    "We will use `pylatexenc` to parse the LaTeX into nodes for custom splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylatexenc.latexwalker import (\n",
    "    LatexWalker,\n",
    "    LatexMacroNode,\n",
    "    LatexEnvironmentNode,\n",
    "    LatexCharsNode,\n",
    "    LatexGroupNode,\n",
    ")\n",
    "\n",
    "START_TOKEN, END_TOKEN = \"<START>\", \"<END>\"\n",
    "\n",
    "\n",
    "# Define the tokenization function using pylatexenc\n",
    "def latex_tokenizer(latex_string):\n",
    "    \"\"\"\n",
    "    Tokenizes a LaTeX string into tokens using pylatexenc.\n",
    "    \"\"\"\n",
    "    if not latex_string:\n",
    "        return []\n",
    "    walker = LatexWalker(latex_string)\n",
    "\n",
    "    def parse_node(nodelist):\n",
    "        if len(nodelist) == 0:\n",
    "            return []\n",
    "        try:\n",
    "            tokens = []\n",
    "            for node in nodelist:\n",
    "                if not node:\n",
    "                    continue\n",
    "                elif node.isNodeType(LatexMacroNode):\n",
    "                    tokens.append(f\"\\\\{node.macroname}\")\n",
    "                    # Parse arguments if they exist\n",
    "                    tokens += parse_node(node.nodeargd.argnlist)\n",
    "                elif node.isNodeType(LatexEnvironmentNode):\n",
    "                    tokens.append(f\"\\\\begin{{{node.environmentname}}}\")\n",
    "                    tokens += parse_node(node.nodeargd.argnlist)\n",
    "                    tokens += parse_node(node.nodelist)\n",
    "                    tokens.append(f\"\\\\end{{{node.environmentname}}}\")\n",
    "                elif node.isNodeType(LatexCharsNode):\n",
    "                    tokens += list(node.chars)\n",
    "                elif node.isNodeType(LatexGroupNode):\n",
    "                    tokens.append(node.delimiters[0])\n",
    "                    tokens += parse_node(node.nodelist)\n",
    "                    tokens.append(node.delimiters[1])\n",
    "            return tokens\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "    nodelist, _, _ = walker.get_latex_nodes()\n",
    "    return parse_node(nodelist)\n",
    "\n",
    "\n",
    "# Wrap the tokenizer for use in TextVectorization\n",
    "def tokenize_fn(latex_tensor):\n",
    "    tokens = []\n",
    "    for latex_string in latex_tensor:\n",
    "        tokenized_string = latex_tokenizer(latex_string.numpy().decode(\"utf-8\"))\n",
    "        tokenized_string.insert(0, START_TOKEN)\n",
    "        tokenized_string.append(END_TOKEN)\n",
    "        tokens.append(tokenized_string)\n",
    "    return tf.ragged.constant(tokens, dtype=tf.string)\n",
    "\n",
    "\n",
    "# Create a TensorFlow-compatible wrapper\n",
    "def tf_tokenizer(latex_string):\n",
    "    return tf.py_function(\n",
    "        func=tokenize_fn,\n",
    "        inp=[latex_string],\n",
    "        Tout=tf.RaggedTensorSpec([None, None], dtype=tf.string),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vectorizer and use a vocabulary file to adapt it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-15 20:29:08.749430: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Create the TextVectorization layer\n",
    "max_tokens = 10_000  # Adjust depending on your vocabulary size\n",
    "\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    standardize=None,  # Custom tokenizer, so no built-in preprocessing\n",
    "    split=tf_tokenizer,\n",
    "    ragged=True,\n",
    ")\n",
    "dataset = tf.data.TextLineDataset(\"vocabulary.txt\")\n",
    "dataset = dataset.map(lambda line: [line])\n",
    "vectorizer.adapt(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out to make sure it works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[4, 60, 45, 11, 45, 30, 39, 9, 12, 5],\n",
      " [4, 20, 3, 21, 2, 3, 38, 2, 45, 18, 45, 79, 3, 39, 2, 5],\n",
      " [4, 73, 6, 3, 19, 11, 10, 2, 9, 16, 45, 19, 9, 12, 45, 11, 45, 20, 3, 16,\n",
      "  7, 16, 18, 10, 8, 7, 12, 16, 18, 10, 8, 2, 3, 92, 2, 5]                 ,\n",
      " [4, 34, 45, 11, 45, 78, 27, 9, 12, 5],\n",
      " [4, 76, 11, 165, 10, 154, 10, 17, 154, 17, 75, 45, 245, 245, 245, 76, 9, 3,\n",
      "  69, 2, 75, 45, 164, 5]                                                    ]>\n",
      "<START>E = mc^2<END>\n"
     ]
    }
   ],
   "source": [
    "def latex_to_token(string):\n",
    "    return vectorizer(string)\n",
    "\n",
    "\n",
    "id_to_token = {i: token for i, token in enumerate(vectorizer.get_vocabulary())}\n",
    "\n",
    "\n",
    "def token_to_latex(tokens):\n",
    "    return \"\".join([id_to_token[id] for id in tokens.numpy()])\n",
    "\n",
    "\n",
    "latex_array = [\n",
    "    r\"E = mc^2\",\n",
    "    r\"\\frac{a}{b} + \\sqrt{c}\",\n",
    "    r\"\\sum_{i=1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}\",\n",
    "    r\"A = \\pi r^2\",\n",
    "    r\"G=\\begin{bmatrix}1&\\dots&1&0&\\dots&0\\\\ \\ast&\\ast&\\ast&&G^{\\prime}&\\\\ \\end{bmatrix}\",\n",
    "]\n",
    "latex_data = tf.constant(latex_array)\n",
    "\n",
    "# Tokenize and vectorize\n",
    "tokenized_output = latex_to_token(latex_data)\n",
    "print(tokenized_output)\n",
    "\n",
    "E_mc2 = token_to_latex(tokenized_output[0])\n",
    "print(E_mc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the vocabulary just in case. You would still need the custom latex parser function imported to use it in another project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vectorizer_vocabulary.txt\", \"w\") as f:\n",
    "    for word in vectorizer.get_vocabulary():\n",
    "        f.write(word)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To re-create the vectorizer using the vocabulary, run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 10_000\n",
    "vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    standardize=None,  # Custom tokenizer, so no built-in preprocessing\n",
    "    split=tf_tokenizer,\n",
    "    ragged=True,\n",
    ")\n",
    "with open(\"vectorizer_vocabulary.txt\", \"r\") as f:\n",
    "    lines = [line[:-1] for line in f]\n",
    "    vectorizer.set_vocabulary(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out to make sure it works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[4, 60, 45, 11, 45, 30, 39, 9, 12, 5],\n",
      " [4, 20, 3, 21, 2, 3, 38, 2, 45, 18, 45, 79, 3, 39, 2, 5],\n",
      " [4, 73, 6, 3, 19, 11, 10, 2, 9, 16, 45, 19, 9, 12, 45, 11, 45, 20, 3, 16,\n",
      "  7, 16, 18, 10, 8, 7, 12, 16, 18, 10, 8, 2, 3, 92, 2, 5]                 ,\n",
      " [4, 34, 45, 11, 45, 78, 27, 9, 12, 5],\n",
      " [4, 76, 11, 165, 10, 154, 10, 17, 154, 17, 75, 45, 245, 245, 245, 76, 9, 3,\n",
      "  69, 2, 75, 45, 164, 5]                                                    ]>\n",
      "<START>E = mc^2<END>\n"
     ]
    }
   ],
   "source": [
    "def latex_to_token(string):\n",
    "    return vectorizer(string)\n",
    "\n",
    "\n",
    "id_to_token = {i: token for i, token in enumerate(vectorizer.get_vocabulary())}\n",
    "\n",
    "\n",
    "def token_to_latex(tokens):\n",
    "    return \"\".join([id_to_token[id] for id in tokens.numpy()])\n",
    "\n",
    "\n",
    "latex_array = [\n",
    "    r\"E = mc^2\",\n",
    "    r\"\\frac{a}{b} + \\sqrt{c}\",\n",
    "    r\"\\sum_{i=1}^n i^2 = \\frac{n(n+1)(2n+1)}{6}\",\n",
    "    r\"A = \\pi r^2\",\n",
    "    r\"G=\\begin{bmatrix}1&\\dots&1&0&\\dots&0\\\\ \\ast&\\ast&\\ast&&G^{\\prime}&\\\\ \\end{bmatrix}\",\n",
    "]\n",
    "latex_data = tf.constant(latex_array)\n",
    "\n",
    "# Tokenize and vectorize\n",
    "tokenized_output = latex_to_token(latex_data)\n",
    "print(tokenized_output)\n",
    "\n",
    "E_mc2 = token_to_latex(tokenized_output[0])\n",
    "print(E_mc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try an actual data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[4, 15, 10, 29, 127, 77, 77, 5]]>\n",
      "<START>-1.955<END>\n"
     ]
    }
   ],
   "source": [
    "random_data_point = next(iter(validation.take(1)))\n",
    "tokenized_data_point = latex_to_token([random_data_point['ground_truth'].numpy()])\n",
    "print(tokenized_data_point)\n",
    "detokenized_data_point = token_to_latex(tokenized_data_point[0])\n",
    "print(detokenized_data_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Strokes\n",
    "\n",
    "Instead of images, this model takes in a stream of strokes, such as writing with a stylus on a tablet. Our dataset gives us a list of strokes, and each stroke is itself a list of coordinates [x, y] of the position of the stylus. Both the number of strokes and the length of each strokes changes for every value in our dataset, so we are going to pre-process the stroke data so it will be normalized (scaled to be between 0 and 1), and always fit in a tensor with shape `(64, 64, 2,)`. FOr this, I am using the [Ramer-Douglas-Peucker Algorithm](https://en.wikipedia.org/wiki/Ramer%E2%80%93Douglas%E2%80%93Peucker_algorithm) for polyline decimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_strokes(strokes: tf.RaggedTensor):\n",
    "    # First, scale values to between 0.0 and 1.0\n",
    "    min_vals = tf.reduce_min(strokes, axis=(0, 1))\n",
    "    max_vals = tf.reduce_max(strokes, axis=(0, 1))\n",
    "    normalized_strokes = tf.map_fn(\n",
    "        elems=strokes,\n",
    "        fn=lambda stroke: (stroke - min_vals) / (max_vals - min_vals + 1e-6),\n",
    "    )\n",
    "\n",
    "    def point_line_distance(point, start, end):\n",
    "        \"\"\"\n",
    "        Calculate the perpendicular distance from `point` to the line segment\n",
    "        defined by `start` and `end`.\n",
    "        \"\"\"\n",
    "        # Convert to 3D by adding a zero z-component\n",
    "        point_3d = tf.concat([point, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
    "        start_3d = tf.concat([start, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
    "        end_3d = tf.concat([end, tf.zeros([1], dtype=tf.float32)], axis=0)\n",
    "\n",
    "        # Return the perpendicular distance (norm of the cross product / norm of the line segment)\n",
    "        return tf.norm(tf.linalg.cross(end_3d - start_3d, point_3d - start_3d)) / tf.norm(end_3d - start_3d)\n",
    "\n",
    "    def douglas_peucker(stroke, epsilon=0.02):\n",
    "        \"\"\"\n",
    "        Non-recursive Douglas-Peucker algorithm implementation.\n",
    "        \"\"\"\n",
    "        stroke_len = tf.shape(stroke)[0]\n",
    "        if stroke_len < 3:\n",
    "            return stroke\n",
    "\n",
    "        # Initialize the list of points to keep\n",
    "        simplified_stroke = [stroke[0]]\n",
    "\n",
    "        # Stack for processing: Each entry contains a tuple (start_index, end_index)\n",
    "        stack = [(0, stroke_len - 1)]\n",
    "\n",
    "        while stack:\n",
    "            start_idx, end_idx = stack.pop()\n",
    "\n",
    "            # Get the relevant slice of the stroke\n",
    "            sub_stroke = stroke[start_idx : end_idx + 1]\n",
    "\n",
    "            # Calculate the perpendicular distances of all intermediate points\n",
    "            start, end = sub_stroke[0], sub_stroke[-1]\n",
    "            distances = tf.vectorized_map(\n",
    "                elems=sub_stroke[1:-1], fn=lambda p: point_line_distance(p, start, end)\n",
    "            )\n",
    "\n",
    "            # Find the point with the maximum distance\n",
    "            if tf.size(distances) > 0:\n",
    "                max_distance = tf.reduce_max(distances)\n",
    "                max_idx = tf.argmax(distances) + 1  # +1 because we skip the start point\n",
    "\n",
    "                # If the max distance is greater than epsilon, continue splitting\n",
    "                if max_distance > epsilon:\n",
    "                    stack.append((start_idx, start_idx + max_idx))\n",
    "                    stack.append((start_idx + max_idx, end_idx))\n",
    "                else:\n",
    "                    # Otherwise, keep the start and end points\n",
    "                    simplified_stroke.append(end)\n",
    "            else:\n",
    "                # If no intermediate points exist, just keep the start and end points\n",
    "                simplified_stroke.append(end)\n",
    "\n",
    "        # Return the simplified stroke\n",
    "        return tf.stack(simplified_stroke)\n",
    "\n",
    "    downsampled_strokes = tf.map_fn(elems=normalized_strokes, fn=douglas_peucker)\n",
    "    return downsampled_strokes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[[0.0, 0.42909095],\n",
      "  [0.09121891, 0.46363685],\n",
      "  [0.06434508, 0.4181823]] , [[0.21158199, 0.80727303],\n",
      "                              [0.29598787, 0.9999998],\n",
      "                              [0.2853899, 0.030908752]],\n",
      " [[0.36866003, 0.8636362],\n",
      "  [0.36222556, 0.92363656]], [[0.5688872, 0.16181804],\n",
      "                              [0.5794851, 0.20181826],\n",
      "                              [0.52384543, 0.38000065],\n",
      "                              [0.53292966, 0.14545508],\n",
      "                              [0.5571537, 0.09090908]] ,\n",
      " [[0.5897048, 0.10000054],\n",
      "  [0.53103703, 0.9127279],\n",
      "  [0.56548077, 0.94363666],\n",
      "  [0.60181683, 0.823636]]  , [[0.8115065, 0.10181772],\n",
      "                              [0.7180167, 0.7945458],\n",
      "                              [0.7838758, 0.53090864],\n",
      "                              [0.7641939, 0.4509096],\n",
      "                              [0.7316428, 0.47818124],\n",
      "                              [0.72218025, 0.45454538],\n",
      "                              [0.7104465, 0.10000054],\n",
      "                              [0.7558668, 0.047273107]],\n",
      " [[0.99999994, 0.00363714],\n",
      "  [0.93035585, 0.75818133],\n",
      "  [0.97161233, 0.73636407],\n",
      "  [0.99999994, 0.5290915],\n",
      "  [0.971234, 0.41999948],\n",
      "  [0.9242997, 0.40727228],\n",
      "  [0.9061318, 0.040000215]]]>\n",
      "/home/jeshwinprince/Programming/crohme/datasets/crohme_dataset/data/INKML/val/CROHME2023_val/form_5_f_205_E1022.inkml\n",
      "Displaying app now...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_data_point = next(iter(validation.take(1)))\n",
    "print(preprocess_strokes(random_data_point[\"strokes\"]))\n",
    "filepath = random_data_point[\"filepath\"].numpy().decode(\"ascii\")\n",
    "os.system(f\"inkmlviewer {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing datasets\n",
    "\n",
    "Now, we can go through our datasets a preprocess all the data. We will need both our vectorizer and our stroke preprocessor together. Since we are going with an encoder-decoder model, we need input data for the decoder as well, which should be the desired output, but just missing the last token, and with the start token added in front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    input_strokes = preprocess_strokes(data[\"strokes\"])\n",
    "    ground_truth = vectorizer([data[\"ground_truth\"].numpy()])\n",
    "    decoder_input = ground_truth[0][:-1]\n",
    "    decoder_output = ground_truth[0][1:]\n",
    "    return (input_strokes, decoder_input), decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input: <START>C(x,y) = C(x-y)\n",
      "True value: C(x,y) = C(x-y)<END>\n",
      "Decoder input: <START>a=3(\\sqrt{10}-4)\\sqrt{10}/(5-4\\sqrt{10})\n",
      "True value: a=3(\\sqrt{10}-4)\\sqrt{10}/(5-4\\sqrt{10})<END>\n",
      "Decoder input: <START>V=\\frac{1}{ 4} k (x-\\frac{L}{ 2})^2 + \\frac{1}{ 4} k (x+\\frac{L}{ 2})^2\n",
      "True value: V=\\frac{1}{ 4} k (x-\\frac{L}{ 2})^2 + \\frac{1}{ 4} k (x+\\frac{L}{ 2})^2<END>\n",
      "Decoder input: <START>3(\\frac{1}{ 3})\n",
      "True value: 3(\\frac{1}{ 3})<END>\n",
      "Decoder input: <START>b_1+b_2+b_3=0\n",
      "True value: b_1+b_2+b_3=0<END>\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test.shuffle(200_000).take(5)):\n",
    "    pp_data = preprocess_data(data)\n",
    "    print(\"Decoder input:\", token_to_latex(pp_data[0][1]))\n",
    "    print(\"True value:\", token_to_latex(pp_data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = test.map(preprocess_data).shuffle(200_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "train = train.map(preprocess_data).shuffle(200_000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
    "validation = (\n",
    "    validation.map(preprocess_data)\n",
    "    .shuffle(200_000)\n",
    "    .batch(32)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this out to make sure it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'filepath': <tf.Tensor: shape=(), dtype=string, numpy=b'/home/jeshwinprince/Programming/crohme/datasets/crohme_dataset/data/INKML/val/CROHME2023_val/form_5_f_205_E1022.inkml'>, 'ground_truth': <tf.Tensor: shape=(), dtype=string, numpy=b'-1.955'>, 'strokes': <tf.RaggedTensor [[[65.75, 78.25],\n",
      "  [65.75, 78.25],\n",
      "  [65.75, 78.26],\n",
      "  [65.76, 78.27],\n",
      "  [65.78, 78.27],\n",
      "  [65.81, 78.28],\n",
      "  [65.83, 78.27],\n",
      "  [65.85, 78.25],\n",
      "  [65.88, 78.25],\n",
      "  [65.91, 78.24],\n",
      "  [65.96, 78.23],\n",
      "  [66.03, 78.21],\n",
      "  [66.16, 78.19],\n",
      "  [66.36, 78.17],\n",
      "  [66.61, 78.16],\n",
      "  [66.88, 78.16],\n",
      "  [67.18, 78.17],\n",
      "  [67.45, 78.19],\n",
      "  [67.72, 78.25],\n",
      "  [67.96, 78.33],\n",
      "  [68.16, 78.44]], [[71.34, 80.33],\n",
      "                    [71.34, 80.33],\n",
      "                    [71.34, 80.33],\n",
      "                    [71.35, 80.34],\n",
      "                    [71.36, 80.35],\n",
      "                    [71.37, 80.34],\n",
      "                    [71.38, 80.33],\n",
      "                    [71.4, 80.31],\n",
      "                    [71.41, 80.28],\n",
      "                    [71.44, 80.26],\n",
      "                    [71.47, 80.2],\n",
      "                    [71.53, 80.11],\n",
      "                    [71.63, 79.96],\n",
      "                    [71.78, 79.72],\n",
      "                    [71.94, 79.42],\n",
      "                    [72.13, 79.06],\n",
      "                    [72.34, 78.67],\n",
      "                    [72.52, 78.31],\n",
      "                    [72.69, 77.94],\n",
      "                    [72.83, 77.62],\n",
      "                    [72.94, 77.33],\n",
      "                    [73.03, 77.06],\n",
      "                    [73.12, 76.82],\n",
      "                    [73.18, 76.61],\n",
      "                    [73.23, 76.42],\n",
      "                    [73.26, 76.27],\n",
      "                    [73.28, 76.17],\n",
      "                    [73.29, 76.1],\n",
      "                    [73.29, 76.07],\n",
      "                    [73.29, 76.06],\n",
      "                    [73.29, 76.08],\n",
      "                    [73.28, 76.08],\n",
      "                    [73.28, 76.1],\n",
      "                    [73.28, 76.12],\n",
      "                    [73.28, 76.16],\n",
      "                    [73.28, 76.22],\n",
      "                    [73.28, 76.32],\n",
      "                    [73.29, 76.47],\n",
      "                    [73.29, 76.67],\n",
      "                    [73.29, 76.91],\n",
      "                    [73.29, 77.17],\n",
      "                    [73.3, 77.44],\n",
      "                    [73.3, 77.77],\n",
      "                    [73.31, 78.13],\n",
      "                    [73.31, 78.52],\n",
      "                    [73.31, 78.92],\n",
      "                    [73.31, 79.32],\n",
      "                    [73.31, 79.7],\n",
      "                    [73.31, 80.06],\n",
      "                    [73.32, 80.39],\n",
      "                    [73.34, 80.64],\n",
      "                    [73.38, 80.88],\n",
      "                    [73.41, 81.06],\n",
      "                    [73.47, 81.22],\n",
      "                    [73.51, 81.33],\n",
      "                    [73.57, 81.39]], [[75.49, 80.64],\n",
      "                                      [75.49, 80.64],\n",
      "                                      [75.49, 80.64],\n",
      "                                      [75.49, 80.64],\n",
      "                                      [75.48, 80.64],\n",
      "                                      [75.47, 80.64],\n",
      "                                      [75.47, 80.64],\n",
      "                                      [75.47, 80.64],\n",
      "                                      [75.46, 80.65],\n",
      "                                      [75.45, 80.65],\n",
      "                                      [75.44, 80.66],\n",
      "                                      [75.44, 80.67],\n",
      "                                      [75.44, 80.67],\n",
      "                                      [75.43, 80.68],\n",
      "                                      [75.43, 80.69],\n",
      "                                      [75.41, 80.69],\n",
      "                                      [75.4, 80.71],\n",
      "                                      [75.39, 80.73],\n",
      "                                      [75.37, 80.76],\n",
      "                                      [75.35, 80.78],\n",
      "                                      [75.34, 80.81],\n",
      "                                      [75.34, 80.82],\n",
      "                                      [75.34, 80.83],\n",
      "                                      [75.34, 80.83],\n",
      "                                      [75.34, 80.83],\n",
      "                                      [75.35, 80.82],\n",
      "                                      [75.36, 80.81],\n",
      "                                      [75.37, 80.81],\n",
      "                                      [75.38, 80.8],\n",
      "                                      [75.38, 80.78],\n",
      "                                      [75.39, 80.77],\n",
      "                                      [75.4, 80.75],\n",
      "                                      [75.4, 80.74],\n",
      "                                      [75.4, 80.72],\n",
      "                                      [75.41, 80.72],\n",
      "                                      [75.41, 80.7],\n",
      "                                      [75.41, 80.69],\n",
      "                                      [75.41, 80.69],\n",
      "                                      [75.4, 80.69],\n",
      "                                      [75.4, 80.69],\n",
      "                                      [75.39, 80.7],\n",
      "                                      [75.38, 80.7],\n",
      "                                      [75.37, 80.71],\n",
      "                                      [75.36, 80.72],\n",
      "                                      [75.35, 80.72],\n",
      "                                      [75.34, 80.72],\n",
      "                                      [75.34, 80.73],\n",
      "                                      [75.33, 80.73],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.32, 80.72],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.32, 80.73],\n",
      "                                      [75.33, 80.75],\n",
      "                                      [75.33, 80.76],\n",
      "                                      [75.34, 80.78],\n",
      "                                      [75.33, 80.79],\n",
      "                                      [75.34, 80.8],\n",
      "                                      [75.34, 80.8],\n",
      "                                      [75.34, 80.81],\n",
      "                                      [75.35, 80.81],\n",
      "                                      [75.35, 80.8],\n",
      "                                      [75.36, 80.8],\n",
      "                                      [75.37, 80.78],\n",
      "                                      [75.38, 80.78],\n",
      "                                      [75.38, 80.76],\n",
      "                                      [75.4, 80.73],\n",
      "                                      [75.4, 80.72],\n",
      "                                      [75.4, 80.69],\n",
      "                                      [75.4, 80.66],\n",
      "                                      [75.4, 80.64],\n",
      "                                      [75.38, 80.62],\n",
      "                                      [75.37, 80.61],\n",
      "                                      [75.34, 80.62],\n",
      "                                      [75.31, 80.63],\n",
      "                                      [75.27, 80.64],\n",
      "                                      [75.22, 80.67],\n",
      "                                      [75.19, 80.69],\n",
      "                                      [75.15, 80.72],\n",
      "                                      [75.11, 80.75],\n",
      "                                      [75.06, 80.8],\n",
      "                                      [75.03, 80.84],\n",
      "                                      [74.99, 80.9],\n",
      "                                      [74.96, 80.94],\n",
      "                                      [74.95, 80.98],\n",
      "                                      [74.96, 81.02],\n",
      "                                      [74.96, 81.03],\n",
      "                                      [74.97, 81.06],\n",
      "                                      [74.99, 81.06],\n",
      "                                      [75.0, 81.08],\n",
      "                                      [75.03, 81.08],\n",
      "                                      [75.05, 81.07],\n",
      "                                      [75.07, 81.06],\n",
      "                                      [75.12, 81.06],\n",
      "                                      [75.16, 81.05],\n",
      "                                      [75.22, 81.03],\n",
      "                                      [75.27, 81.0],\n",
      "                                      [75.32, 80.97]], [[80.78, 76.78],\n",
      "                                                        [80.8, 76.72],\n",
      "                                                        [80.8, 76.67],\n",
      "                                                        [80.8, 76.62],\n",
      "                                                        [80.79, 76.58],\n",
      "                                                        [80.78, 76.56],\n",
      "                                                        [80.78, 76.53],\n",
      "                                                        [80.75, 76.52],\n",
      "                                                        [80.72, 76.48],\n",
      "                                                        [80.68, 76.45],\n",
      "                                                        [80.62, 76.43],\n",
      "                                                        [80.55, 76.41],\n",
      "                                                        [80.47, 76.39],\n",
      "                                                        [80.37, 76.39],\n",
      "                                                        [80.27, 76.39],\n",
      "                                                        [80.18, 76.41],\n",
      "                                                        [80.07, 76.46],\n",
      "                                                        [79.96, 76.56],\n",
      "                                                        [79.83, 76.69],\n",
      "                                                        [79.7, 76.87],\n",
      "                                                        [79.59, 77.06],\n",
      "                                                        [79.52, 77.22],\n",
      "                                                        [79.47, 77.38],\n",
      "                                                        [79.45, 77.51],\n",
      "                                                        [79.45, 77.62],\n",
      "                                                        [79.46, 77.73],\n",
      "                                                        [79.5, 77.83],\n",
      "                                                        [79.53, 77.91],\n",
      "                                                        [79.59, 77.98],\n",
      "                                                        [79.66, 78.03],\n",
      "                                                        [79.74, 78.05],\n",
      "                                                        [79.83, 78.05],\n",
      "                                                        [79.94, 78.03],\n",
      "                                                        [80.06, 77.99],\n",
      "                                                        [80.21, 77.91],\n",
      "                                                        [80.35, 77.79],\n",
      "                                                        [80.51, 77.64],\n",
      "                                                        [80.66, 77.47],\n",
      "                                                        [80.81, 77.31],\n",
      "                                                        [80.94, 77.14],\n",
      "                                                        [81.06, 77.0]] ,\n",
      " [[81.33, 76.44],\n",
      "  [81.3, 76.42],\n",
      "  [81.27, 76.41],\n",
      "  [81.24, 76.41],\n",
      "  [81.21, 76.41],\n",
      "  [81.19, 76.42],\n",
      "  [81.18, 76.43],\n",
      "  [81.16, 76.44],\n",
      "  [81.16, 76.47],\n",
      "  [81.15, 76.53],\n",
      "  [81.15, 76.61],\n",
      "  [81.14, 76.72],\n",
      "  [81.14, 76.89],\n",
      "  [81.16, 77.09],\n",
      "  [81.2, 77.35],\n",
      "  [81.26, 77.63],\n",
      "  [81.34, 77.92],\n",
      "  [81.43, 78.22],\n",
      "  [81.51, 78.53],\n",
      "  [81.59, 78.88],\n",
      "  [81.66, 79.22],\n",
      "  [81.7, 79.55],\n",
      "  [81.72, 79.84],\n",
      "  [81.72, 80.06],\n",
      "  [81.71, 80.22],\n",
      "  [81.68, 80.33],\n",
      "  [81.65, 80.42],\n",
      "  [81.6, 80.5],\n",
      "  [81.56, 80.58],\n",
      "  [81.51, 80.64],\n",
      "  [81.44, 80.69],\n",
      "  [81.35, 80.75],\n",
      "  [81.22, 80.83],\n",
      "  [81.06, 80.93],\n",
      "  [80.88, 81.02],\n",
      "  [80.69, 81.08],\n",
      "  [80.51, 81.11],\n",
      "  [80.33, 81.11],\n",
      "  [80.14, 81.06],\n",
      "  [79.95, 81.0],\n",
      "  [79.78, 80.91]], [[87.19, 76.45],\n",
      "                    [87.23, 76.42],\n",
      "                    [87.25, 76.42],\n",
      "                    [87.24, 76.42],\n",
      "                    [87.24, 76.42],\n",
      "                    [87.22, 76.42],\n",
      "                    [87.21, 76.42],\n",
      "                    [87.19, 76.42],\n",
      "                    [87.16, 76.42],\n",
      "                    [87.12, 76.42],\n",
      "                    [87.1, 76.42],\n",
      "                    [87.08, 76.41],\n",
      "                    [87.06, 76.41],\n",
      "                    [87.03, 76.39],\n",
      "                    [87.0, 76.39],\n",
      "                    [86.96, 76.38],\n",
      "                    [86.9, 76.36],\n",
      "                    [86.81, 76.33],\n",
      "                    [86.69, 76.3],\n",
      "                    [86.55, 76.27],\n",
      "                    [86.38, 76.22],\n",
      "                    [86.17, 76.19],\n",
      "                    [85.95, 76.17],\n",
      "                    [85.72, 76.15],\n",
      "                    [85.48, 76.16],\n",
      "                    [85.25, 76.18],\n",
      "                    [85.05, 76.22],\n",
      "                    [84.88, 76.27],\n",
      "                    [84.75, 76.32],\n",
      "                    [84.65, 76.36],\n",
      "                    [84.59, 76.41],\n",
      "                    [84.54, 76.43],\n",
      "                    [84.53, 76.44],\n",
      "                    [84.52, 76.44],\n",
      "                    [84.53, 76.44],\n",
      "                    [84.54, 76.44],\n",
      "                    [84.56, 76.44],\n",
      "                    [84.56, 76.44],\n",
      "                    [84.56, 76.44],\n",
      "                    [84.57, 76.45],\n",
      "                    [84.57, 76.47],\n",
      "                    [84.57, 76.51],\n",
      "                    [84.59, 76.58],\n",
      "                    [84.59, 76.65],\n",
      "                    [84.6, 76.74],\n",
      "                    [84.6, 76.85],\n",
      "                    [84.59, 76.98],\n",
      "                    [84.59, 77.11],\n",
      "                    [84.59, 77.27],\n",
      "                    [84.59, 77.43],\n",
      "                    [84.6, 77.58],\n",
      "                    [84.6, 77.72],\n",
      "                    [84.62, 77.86],\n",
      "                    [84.64, 77.97],\n",
      "                    [84.68, 78.09],\n",
      "                    [84.72, 78.2],\n",
      "                    [84.78, 78.31],\n",
      "                    [84.83, 78.39],\n",
      "                    [84.9, 78.45],\n",
      "                    [84.96, 78.49],\n",
      "                    [85.02, 78.51],\n",
      "                    [85.08, 78.52],\n",
      "                    [85.13, 78.51],\n",
      "                    [85.2, 78.5],\n",
      "                    [85.27, 78.5],\n",
      "                    [85.36, 78.47],\n",
      "                    [85.47, 78.44],\n",
      "                    [85.57, 78.41],\n",
      "                    [85.68, 78.38],\n",
      "                    [85.78, 78.36],\n",
      "                    [85.86, 78.36],\n",
      "                    [85.94, 78.37],\n",
      "                    [86.01, 78.39],\n",
      "                    [86.09, 78.42],\n",
      "                    [86.16, 78.45],\n",
      "                    [86.24, 78.5],\n",
      "                    [86.31, 78.56],\n",
      "                    [86.36, 78.62],\n",
      "                    [86.41, 78.69],\n",
      "                    [86.44, 78.75],\n",
      "                    [86.46, 78.81],\n",
      "                    [86.46, 78.86],\n",
      "                    [86.45, 78.89],\n",
      "                    [86.44, 78.92],\n",
      "                    [86.43, 78.96],\n",
      "                    [86.4, 79.0],\n",
      "                    [86.38, 79.04],\n",
      "                    [86.33, 79.09],\n",
      "                    [86.26, 79.17],\n",
      "                    [86.14, 79.31],\n",
      "                    [85.97, 79.48],\n",
      "                    [85.78, 79.67],\n",
      "                    [85.57, 79.85],\n",
      "                    [85.4, 79.98],\n",
      "                    [85.26, 80.09],\n",
      "                    [85.15, 80.19],\n",
      "                    [85.03, 80.29],\n",
      "                    [84.92, 80.39],\n",
      "                    [84.82, 80.46],\n",
      "                    [84.75, 80.5],\n",
      "                    [84.72, 80.48],\n",
      "                    [84.71, 80.41],\n",
      "                    [84.72, 80.26]], [[92.17, 75.91],\n",
      "                                      [92.17, 75.9],\n",
      "                                      [92.17, 75.9],\n",
      "                                      [92.17, 75.89],\n",
      "                                      [92.16, 75.89],\n",
      "                                      [92.15, 75.9],\n",
      "                                      [92.14, 75.91],\n",
      "                                      [92.11, 75.91],\n",
      "                                      [92.09, 75.91],\n",
      "                                      [92.06, 75.9],\n",
      "                                      [92.02, 75.89],\n",
      "                                      [91.94, 75.89],\n",
      "                                      [91.83, 75.89],\n",
      "                                      [91.67, 75.89],\n",
      "                                      [91.5, 75.89],\n",
      "                                      [91.29, 75.91],\n",
      "                                      [91.07, 75.92],\n",
      "                                      [90.84, 75.95],\n",
      "                                      [90.6, 75.99],\n",
      "                                      [90.36, 76.03],\n",
      "                                      [90.14, 76.06],\n",
      "                                      [89.94, 76.09],\n",
      "                                      [89.81, 76.11],\n",
      "                                      [89.72, 76.11],\n",
      "                                      [89.69, 76.11],\n",
      "                                      [89.69, 76.11],\n",
      "                                      [89.69, 76.11],\n",
      "                                      [89.69, 76.11],\n",
      "                                      [89.7, 76.1],\n",
      "                                      [89.71, 76.1],\n",
      "                                      [89.72, 76.09],\n",
      "                                      [89.73, 76.09],\n",
      "                                      [89.74, 76.1],\n",
      "                                      [89.75, 76.11],\n",
      "                                      [89.75, 76.11],\n",
      "                                      [89.76, 76.14],\n",
      "                                      [89.78, 76.18],\n",
      "                                      [89.79, 76.23],\n",
      "                                      [89.81, 76.31],\n",
      "                                      [89.83, 76.43],\n",
      "                                      [89.86, 76.56],\n",
      "                                      [89.89, 76.72],\n",
      "                                      [89.91, 76.88],\n",
      "                                      [89.94, 77.04],\n",
      "                                      [89.97, 77.23],\n",
      "                                      [90.0, 77.42],\n",
      "                                      [90.03, 77.59],\n",
      "                                      [90.05, 77.74],\n",
      "                                      [90.07, 77.84],\n",
      "                                      [90.09, 77.94],\n",
      "                                      [90.11, 78.0],\n",
      "                                      [90.13, 78.06],\n",
      "                                      [90.15, 78.1],\n",
      "                                      [90.17, 78.13],\n",
      "                                      [90.2, 78.16],\n",
      "                                      [90.24, 78.18],\n",
      "                                      [90.28, 78.19],\n",
      "                                      [90.33, 78.2],\n",
      "                                      [90.38, 78.21],\n",
      "                                      [90.42, 78.2],\n",
      "                                      [90.46, 78.2],\n",
      "                                      [90.5, 78.19],\n",
      "                                      [90.56, 78.19],\n",
      "                                      [90.63, 78.18],\n",
      "                                      [90.7, 78.17],\n",
      "                                      [90.8, 78.16],\n",
      "                                      [90.89, 78.16],\n",
      "                                      [90.98, 78.16],\n",
      "                                      [91.09, 78.16],\n",
      "                                      [91.19, 78.16],\n",
      "                                      [91.3, 78.17],\n",
      "                                      [91.41, 78.2],\n",
      "                                      [91.52, 78.25],\n",
      "                                      [91.63, 78.29],\n",
      "                                      [91.75, 78.34],\n",
      "                                      [91.85, 78.39],\n",
      "                                      [91.95, 78.47],\n",
      "                                      [92.04, 78.55],\n",
      "                                      [92.1, 78.62],\n",
      "                                      [92.14, 78.69],\n",
      "                                      [92.16, 78.75],\n",
      "                                      [92.17, 78.8],\n",
      "                                      [92.17, 78.86],\n",
      "                                      [92.16, 78.92],\n",
      "                                      [92.16, 79.0],\n",
      "                                      [92.13, 79.09],\n",
      "                                      [92.09, 79.17],\n",
      "                                      [92.06, 79.26],\n",
      "                                      [92.0, 79.34],\n",
      "                                      [91.92, 79.44],\n",
      "                                      [91.81, 79.57],\n",
      "                                      [91.67, 79.72],\n",
      "                                      [91.54, 79.84],\n",
      "                                      [91.42, 79.94],\n",
      "                                      [91.29, 80.0],\n",
      "                                      [91.15, 80.04],\n",
      "                                      [90.98, 80.08],\n",
      "                                      [90.78, 80.11],\n",
      "                                      [90.56, 80.11],\n",
      "                                      [90.33, 80.06]]]>}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(validation.take(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "My model is an encoder-decoder architecture, with a CNN for the encoder, a feedforward network to get to the latent space, and a LSTM RNN for the decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_strokes = layers.Input(shape=(64, 64, 2))\n",
    "x = layers.Conv2D(64, kernel_size=(3, 3), padding=\"same\")(input_strokes)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(128, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(256, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(512, kernel_size=(3, 3), padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "\n",
    "latent_space = layers.Dense(1024, activation=\"relu\")(x)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)\n",
    "latent_space = layers.Dense(512, activation=\"relu\")(latent_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_h = layers.Dense(256, activation=\"relu\")(latent_space)\n",
    "latent_space_c = layers.Dense(256, activation=\"relu\")(latent_space)\n",
    "\n",
    "decoder_input = layers.Input(shape=(None,))\n",
    "decoder_embedding = layers.Embedding(input_dim=vectorizer.vocabulary_size(), output_dim=128)(\n",
    "    decoder_input\n",
    ")\n",
    "decoder_lstm = layers.LSTM(128, return_sequences=True)\n",
    "decoder_output = decoder_lstm(\n",
    "    decoder_embedding, initial_state=[latent_space_h, latent_space_c]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = layers.Dense(vectorizer.vocabulary_size(), activation=\"softmax\")(decoder_output)\n",
    "model = keras.Model([input_strokes, decoder_input], output)\n",
    "model.compile(\n",
    "    optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now we can finally train our model! We have a ton of training and validation data to use. We'll also save the history so we can get a graph of the change in loss over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=validation,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\"model_checkpoint.h5\", save_best_only=True),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
